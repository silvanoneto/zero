#!/usr/bin/env python3
"""
Script de normaliza√ß√£o da distribui√ß√£o de conectividade
Ajusta conex√µes para aproximar distribui√ß√£o gaussiana (normal)
"""

import json
import statistics
import random
from collections import defaultdict
from pathlib import Path

# Caminhos
BASE_DIR = Path(__file__).parent.parent
CONCEPTS_FILE = BASE_DIR / 'assets' / 'concepts.json'
RELATIONS_FILE = BASE_DIR / 'assets' / 'relations.json'


def load_json(filepath):
    """Carrega arquivo JSON com encoding UTF-8"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_json(filepath, data):
    """Salva arquivo JSON com encoding UTF-8 e formata√ß√£o"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_concept_by_id(concept_id, concepts):
    """Retorna conceito por ID"""
    for c in concepts:
        if c['id'] == concept_id:
            return c
    return None


def analyze_distribution(concepts):
    """Analisa distribui√ß√£o de conectividade"""
    degrees = [len(c['connections']) for c in concepts]
    mean = statistics.mean(degrees)
    stdev = statistics.stdev(degrees)
    
    return {
        'mean': mean,
        'median': statistics.median(degrees),
        'stdev': stdev,
        'min': min(degrees),
        'max': max(degrees),
        'range_1sigma': (mean - stdev, mean + stdev),
        'range_2sigma': (mean - 2*stdev, mean + 2*stdev)
    }


def classify_concepts(concepts, stats):
    """Classifica conceitos por n√≠vel de conectividade"""
    under_connected = []
    normal_connected = []
    over_connected = []
    
    for c in concepts:
        degree = len(c['connections'])
        if degree < stats['range_1sigma'][0]:
            under_connected.append(c)
        elif degree > stats['range_1sigma'][1]:
            over_connected.append(c)
        else:
            normal_connected.append(c)
    
    return {
        'under': sorted(under_connected, key=lambda x: len(x['connections'])),
        'normal': normal_connected,
        'over': sorted(over_connected, key=lambda x: -len(x['connections']))
    }


def find_compatible_connections(concept, concepts, stats, max_suggestions=10):
    """
    Encontra conceitos compat√≠veis para novas conex√µes
    Crit√©rios:
    - Mesma camada ou camadas adjacentes
    - N√£o j√° conectados
    - Prefer√™ncia por conceitos sub-conectados ou normais
    """
    same_layer = concept.get('layer')
    current_connections = set(concept['connections'])
    current_id = concept['id']
    
    # Camadas adjacentes l√≥gicas
    layer_adjacency = {
        'fundacional': ['ontologica', 'temporal'],
        'ontologica': ['fundacional', 'epistemica', 'ecologica'],
        'epistemica': ['ontologica', 'politica', 'etica'],
        'politica': ['epistemica', 'pratica', 'etica'],
        'etica': ['epistemica', 'politica', 'pratica'],
        'temporal': ['fundacional', 'ontologica', 'pratica'],
        'ecologica': ['ontologica', 'pratica', 'etica'],
        'pratica': ['politica', 'etica', 'temporal', 'ecologica']
    }
    
    compatible_layers = [same_layer] + layer_adjacency.get(same_layer, [])
    
    candidates = []
    for c in concepts:
        if c['id'] == current_id:
            continue
        if c['id'] in current_connections:
            continue
        
        # Prefer√™ncia por mesma camada
        layer_score = 2 if c.get('layer') == same_layer else 1 if c.get('layer') in compatible_layers else 0.5
        
        # Prefer√™ncia por conceitos sub-conectados
        degree = len(c['connections'])
        if degree < stats['range_1sigma'][0]:
            connectivity_score = 3  # Sub-conectado (prioridade alta)
        elif degree <= stats['range_1sigma'][1]:
            connectivity_score = 2  # Normal
        else:
            connectivity_score = 1  # Sobre-conectado (baixa prioridade)
        
        score = layer_score * connectivity_score
        candidates.append((c, score))
    
    # Ordena por score e retorna top N
    candidates.sort(key=lambda x: -x[1])
    return [c for c, score in candidates[:max_suggestions]]


def suggest_verb_for_connection(from_concept, to_concept):
    """Sugere verbo apropriado baseado nas camadas"""
    from_layer = from_concept.get('layer', 'desconhecida')
    to_layer = to_concept.get('layer', 'desconhecida')
    
    # Mapeamento de verbos por camada
    layer_verbs = {
        'fundacional': ['fundamenta-se em', 'emerge de', 'sustenta-se em', 'condiciona'],
        'ontologica': ['constitui', 'articula-se com', 'entrela√ßa-se com', 'co-constitui'],
        'epistemica': ['conhece atrav√©s de', 'aprende de', 'questiona', 'dialoga com'],
        'politica': ['mobiliza', 'articula-se politicamente com', 'resiste a', 'organiza-se em'],
        'etica': ['cuida de', 'responsabiliza-se por', 'orienta-se eticamente por', 'respeita'],
        'temporal': ['desdobra-se em', 'temporaliza-se em', 'evolui para', 'atualiza'],
        'ecologica': ['simbiosa com', 'co-habita', 'flui em', 'entrela√ßa-se ecologicamente com'],
        'pratica': ['pratica', 'implementa', 'performa', 'efetiva-se em']
    }
    
    # Mesma camada - usa verbo da camada
    if from_layer == to_layer and from_layer in layer_verbs:
        return random.choice(layer_verbs[from_layer])
    
    # Camadas diferentes - verbo da camada de origem
    if from_layer in layer_verbs:
        return random.choice(layer_verbs[from_layer])
    
    # Fallback
    return 'relaciona-se com'


def create_new_relation(from_concept, to_concept, relations):
    """Cria nova rela√ß√£o entre conceitos"""
    verb = suggest_verb_for_connection(from_concept, to_concept)
    
    relation = {
        'from': from_concept['id'],
        'to': to_concept['id'],
        'name': verb,
        'description': f"{from_concept['name']} {verb} {to_concept['name']}"
    }
    
    return relation


def normalize_connectivity(concepts, relations, target_additions=100, dry_run=False):
    """
    Normaliza distribui√ß√£o de conectividade
    Adiciona conex√µes estrat√©gicas para aproximar distribui√ß√£o normal
    """
    print("üéØ NORMALIZA√á√ÉO DA DISTRIBUI√á√ÉO DE CONECTIVIDADE")
    print("=" * 70)
    
    # Analisa distribui√ß√£o atual
    stats_before = analyze_distribution(concepts)
    classified = classify_concepts(concepts, stats_before)
    
    print(f"\nüìä ESTADO ATUAL:")
    print(f"   Œº (m√©dia):     {stats_before['mean']:.2f}")
    print(f"   œÉ (desvio):    {stats_before['stdev']:.2f}")
    print(f"   Faixa ¬±1œÉ:     {stats_before['range_1sigma'][0]:.1f} - {stats_before['range_1sigma'][1]:.1f}")
    print(f"\n   Sub-conectados:    {len(classified['under']):3d} ({len(classified['under'])/len(concepts)*100:4.1f}%)")
    print(f"   Bem conectados:    {len(classified['normal']):3d} ({len(classified['normal'])/len(concepts)*100:4.1f}%)")
    print(f"   Sobre-conectados:  {len(classified['over']):3d} ({len(classified['over'])/len(concepts)*100:4.1f}%)")
    
    # Prioriza conceitos mais sub-conectados
    concepts_to_enhance = classified['under'][:target_additions]
    
    print(f"\nüîÑ PLANEJAMENTO:")
    print(f"   Conceitos a melhorar: {len(concepts_to_enhance)}")
    print(f"   Conex√µes a adicionar: ~{target_additions * 2} (bidirecional)")
    
    if dry_run:
        print(f"\n‚ö†Ô∏è  MODO DRY-RUN: Simulando mudan√ßas...")
    
    # Cria novas conex√µes
    new_relations = []
    connections_added = defaultdict(int)
    
    for concept in concepts_to_enhance:
        # Determina quantas conex√µes adicionar
        current_degree = len(concept['connections'])
        target_degree = int(stats_before['mean'])
        additions_needed = max(1, target_degree - current_degree)
        
        # Encontra candidatos compat√≠veis
        candidates = find_compatible_connections(concept, concepts, stats_before, 
                                                 max_suggestions=additions_needed * 2)
        
        added = 0
        for candidate in candidates[:additions_needed]:
            # Cria rela√ß√£o bidirecional
            rel1 = create_new_relation(concept, candidate, relations)
            rel2 = create_new_relation(candidate, concept, relations)
            
            new_relations.append(rel1)
            new_relations.append(rel2)
            
            # Atualiza conex√µes nos conceitos (em mem√≥ria)
            if candidate['id'] not in concept['connections']:
                concept['connections'].append(candidate['id'])
            if concept['id'] not in candidate['connections']:
                candidate['connections'].append(concept['id'])
            
            connections_added[concept['id']] += 1
            connections_added[candidate['id']] += 1
            added += 1
        
        if added > 0 and len(new_relations) % 20 == 0:
            print(f"   ‚Ä¢ Processados: {len(connections_added)} conceitos, {len(new_relations)} rela√ß√µes criadas")
    
    print(f"\n‚úÖ Processamento conclu√≠do:")
    print(f"   ‚Ä¢ Conceitos melhorados: {len(connections_added)}")
    print(f"   ‚Ä¢ Novas rela√ß√µes: {len(new_relations)}")
    
    # Analisa nova distribui√ß√£o
    stats_after = analyze_distribution(concepts)
    classified_after = classify_concepts(concepts, stats_after)
    
    print(f"\nüìä ESTADO AP√ìS NORMALIZA√á√ÉO:")
    print(f"   Œº (m√©dia):     {stats_after['mean']:.2f} (antes: {stats_before['mean']:.2f})")
    print(f"   œÉ (desvio):    {stats_after['stdev']:.2f} (antes: {stats_before['stdev']:.2f})")
    print(f"\n   Sub-conectados:    {len(classified_after['under']):3d} ({len(classified_after['under'])/len(concepts)*100:4.1f}%) - antes: {len(classified['under'])}")
    print(f"   Bem conectados:    {len(classified_after['normal']):3d} ({len(classified_after['normal'])/len(concepts)*100:4.1f}%) - antes: {len(classified['normal'])}")
    print(f"   Sobre-conectados:  {len(classified_after['over']):3d} ({len(classified_after['over'])/len(concepts)*100:4.1f}%) - antes: {len(classified['over'])}")
    
    # Conformidade com normal
    in_1sigma = len(classified_after['normal']) / len(concepts) * 100
    improvement = in_1sigma - (len(classified['normal']) / len(concepts) * 100)
    
    print(f"\nüìê CONFORMIDADE COM NORMAL:")
    print(f"   Dentro de ¬±1œÉ: {in_1sigma:.1f}% (ideal: 68%, melhoria: +{improvement:.1f}%)")
    
    if abs(in_1sigma - 68) < 5:
        print(f"   ‚úÖ EXCELENTE conformidade alcan√ßada!")
    elif abs(in_1sigma - 68) < 10:
        print(f"   ‚úÖ BOA conformidade alcan√ßada")
    else:
        print(f"   ‚ö†Ô∏è  Conformidade melhorou, mas ainda pode ser otimizada")
    
    # Salva ou simula
    if not dry_run:
        print(f"\nüíæ Salvando mudan√ßas...")
        save_json(CONCEPTS_FILE, concepts)
        
        # Adiciona novas rela√ß√µes
        all_relations = relations + new_relations
        save_json(RELATIONS_FILE, all_relations)
        
        print(f"   ‚úÖ Conceitos atualizados: {CONCEPTS_FILE}")
        print(f"   ‚úÖ Rela√ß√µes atualizadas: {RELATIONS_FILE}")
    else:
        print(f"\n‚ö†Ô∏è  DRY-RUN: Nenhuma mudan√ßa foi salva")
        print(f"   Execute sem --dry-run para aplicar mudan√ßas")
    
    return {
        'before': stats_before,
        'after': stats_after,
        'new_relations': len(new_relations),
        'concepts_enhanced': len(connections_added)
    }


def main():
    """Fun√ß√£o principal"""
    import sys
    
    # Op√ß√µes de linha de comando
    dry_run = '--dry-run' in sys.argv
    target = 100
    
    for arg in sys.argv:
        if arg.startswith('--target='):
            target = int(arg.split('=')[1])
    
    print("üîß NORMALIZA√á√ÉO DE CONECTIVIDADE DA ONTOLOGIA CRIOS")
    print("=" * 70)
    
    if dry_run:
        print("\n‚ö†Ô∏è  MODO SIMULA√á√ÉO - Nenhuma mudan√ßa ser√° salva")
    
    # Carrega dados
    print("\nüìÇ Carregando dados...")
    concepts = load_json(CONCEPTS_FILE)
    relations = load_json(RELATIONS_FILE)
    
    print(f"   ‚Ä¢ {len(concepts)} conceitos")
    print(f"   ‚Ä¢ {len(relations)} rela√ß√µes")
    
    # Normaliza
    result = normalize_connectivity(concepts, relations, target_additions=target, dry_run=dry_run)
    
    print("\n" + "=" * 70)
    if not dry_run:
        print("‚ú® Normaliza√ß√£o conclu√≠da com sucesso!")
        print("\nüí° Execute 'make validate' para verificar integridade")
        print("üí° Execute 'make stats-full' para ver nova distribui√ß√£o")
    else:
        print("‚ú® Simula√ß√£o conclu√≠da!")
        print("\nüí° Execute sem --dry-run para aplicar mudan√ßas")


if __name__ == '__main__':
    main()
