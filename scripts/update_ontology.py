#!/usr/bin/env python3
"""
Script de atualiza√ß√£o e valida√ß√£o completa da ontologia CRIOS
Executa verifica√ß√µes de qualidade, mesclas, valida√ß√µes e estat√≠sticas
"""

import json
import sys
from collections import Counter, defaultdict
from pathlib import Path

# Caminhos dos arquivos
BASE_DIR = Path(__file__).parent.parent
CONCEPTS_FILE = BASE_DIR / 'assets' / 'concepts.json'
RELATIONS_FILE = BASE_DIR / 'assets' / 'relations.json'
REFERENCIAS_FILE = BASE_DIR / 'assets' / 'referencias.json'

# Camadas ontol√≥gicas can√¥nicas
CANONICAL_LAYERS = {
    'fundacional', 'ontologica', 'epistemica', 'politica',
    'etica', 'temporal', 'ecologica', 'pratica'
}

# Verbos por camada para gera√ß√£o de rela√ß√µes
LAYER_VERBS = {
    'fundacional': [
        'fundamenta', 'sustenta', 'origina', 'possibilita', 'condiciona',
        'precede', 'emerge em', 'dissolve-se em', 'manifesta-se em'
    ],
    'ontologica': [
        'constitui', 'articula-se com', 'entrela√ßa-se com', 'comp√µe',
        'estrutura', 'configura-se em', 'realiza-se em', 'intensifica-se em'
    ],
    'epistemica': [
        'informa', 'revela', 'questiona', 'problematiza', 'reconhece',
        'aprende de', 'pensa atrav√©s de', 'dialoga com'
    ],
    'politica': [
        'mobiliza', 'resiste a', 'organiza', 'transforma', 'radicaliza-se em',
        'politiza-se em', 'emancipa-se via', 'disputa'
    ],
    'etica': [
        'responsabiliza-se por', 'cuida de', 'respeita', 'protege',
        'compartilha', 'reconhece', 'acolhe'
    ],
    'temporal': [
        'desdobra-se em', 'evolui para', 'sedimenta-se em', 'projeta-se em',
        'atualiza', 'dev√©m', 'marca'
    ],
    'ecologica': [
        'simbiosa com', 'co-habita', 'entrela√ßa-se com', 'circula em',
        'flui em', 'enra√≠za-se em', 'territorializa-se em'
    ],
    'pratica': [
        'implementa', 'pratica', 'performa', 'efetiva', 'atua em',
        'realiza', 'experimenta', 'opera'
    ]
}


def load_json(filepath):
    """Carrega arquivo JSON com encoding UTF-8"""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_json(filepath, data):
    """Salva arquivo JSON com encoding UTF-8 e formata√ß√£o"""
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def print_section(title):
    """Imprime se√ß√£o formatada"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print(f"{'='*60}\n")


def check_duplicates(concepts):
    """Verifica duplica√ß√µes de ID e nome"""
    print_section("1. VERIFICA√á√ÉO DE DUPLICATAS")
    
    ids = [c['id'] for c in concepts]
    names = [c['name'] for c in concepts]
    
    id_counts = Counter(ids)
    name_counts = Counter(names)
    
    id_dups = {k: v for k, v in id_counts.items() if v > 1}
    name_dups = {k: v for k, v in name_counts.items() if v > 1}
    
    if id_dups:
        print(f"‚ùå {len(id_dups)} IDs duplicados:")
        for dup_id, count in id_dups.items():
            print(f"   ‚Ä¢ {dup_id} ({count}√ó)")
    else:
        print("‚úÖ Nenhum ID duplicado")
    
    if name_dups:
        print(f"‚ùå {len(name_dups)} nomes duplicados:")
        for dup_name, count in name_dups.items():
            print(f"   ‚Ä¢ {dup_name} ({count}√ó)")
    else:
        print("‚úÖ Nenhum nome duplicado")
    
    return len(id_dups) == 0 and len(name_dups) == 0


def check_required_fields(concepts):
    """Verifica campos obrigat√≥rios"""
    print_section("2. VERIFICA√á√ÉO DE CAMPOS OBRIGAT√ìRIOS")
    
    critical_errors = []
    warnings = []
    
    for c in concepts:
        if not c.get('id'):
            critical_errors.append(f"‚ùå Sem ID: {c.get('name', 'SEM NOME')}")
        if not c.get('name'):
            critical_errors.append(f"‚ùå Sem nome: {c.get('id', 'SEM ID')}")
        if not c.get('description') or len(c.get('description', '')) < 20:
            warnings.append(f"‚ö†Ô∏è  Descri√ß√£o curta (<20 chars): {c['id']}")
        if not c.get('layer'):
            critical_errors.append(f"‚ùå Sem camada: {c['id']}")
        if not c.get('connections') or len(c.get('connections', [])) == 0:
            warnings.append(f"‚ö†Ô∏è  Sem conex√µes: {c['id']}")
    
    if critical_errors:
        print(f"‚ùå {len(critical_errors)} erros cr√≠ticos encontrados:")
        for issue in critical_errors[:20]:
            print(f"   {issue}")
        if len(critical_errors) > 20:
            print(f"   ... e mais {len(critical_errors) - 20} erros")
    
    if warnings:
        print(f"‚ö†Ô∏è  {len(warnings)} avisos encontrados:")
        for issue in warnings[:10]:
            print(f"   {issue}")
        if len(warnings) > 10:
            print(f"   ... e mais {len(warnings) - 10} avisos")
    
    if not critical_errors and not warnings:
        print("‚úÖ Todos os conceitos t√™m campos obrigat√≥rios")
    
    # Only fail on critical errors, not warnings
    return len(critical_errors) == 0


def check_orphan_references(concepts):
    """Verifica refer√™ncias √≥rf√£s nas conex√µes"""
    print_section("3. VERIFICA√á√ÉO DE REFER√äNCIAS √ìRF√ÉS")
    
    all_ids = set(c['id'] for c in concepts)
    orphan_refs = set()
    
    for c in concepts:
        for conn in c.get('connections', []):
            if conn not in all_ids:
                orphan_refs.add(f"{c['id']} ‚Üí {conn}")
    
    if orphan_refs:
        print(f"‚ùå {len(orphan_refs)} refer√™ncias √≥rf√£s:")
        for ref in sorted(list(orphan_refs))[:20]:
            print(f"   {ref}")
        if len(orphan_refs) > 20:
            print(f"   ... e mais {len(orphan_refs) - 20} refer√™ncias")
    else:
        print("‚úÖ Todas as conex√µes s√£o v√°lidas (0 √≥rf√£s)")
    
    return len(orphan_refs) == 0


def analyze_layer_distribution(concepts):
    """Analisa distribui√ß√£o por camada"""
    print_section("4. DISTRIBUI√á√ÉO POR CAMADA")
    
    layer_dist = Counter(c.get('layer') for c in concepts)
    
    print(f"Total de conceitos: {len(concepts)}\n")
    for layer in CANONICAL_LAYERS:
        count = layer_dist.get(layer, 0)
        pct = (count / len(concepts) * 100) if concepts else 0
        print(f"  {layer:12s}: {count:3d} ({pct:5.1f}%)")
    
    # Camadas n√£o-can√¥nicas
    non_canonical = {k: v for k, v in layer_dist.items() if k not in CANONICAL_LAYERS}
    if non_canonical:
        print(f"\n‚ö†Ô∏è  Camadas n√£o-can√¥nicas encontradas:")
        for layer, count in non_canonical.items():
            print(f"  {layer}: {count}")


def analyze_connections(concepts):
    """Analisa estat√≠sticas de conex√µes"""
    print_section("5. ESTAT√çSTICAS DE CONEX√ïES")
    
    conn_counts = [len(c.get('connections', [])) for c in concepts]
    
    if conn_counts:
        avg = sum(conn_counts) / len(conn_counts)
        print(f"  M√©dia:   {avg:.1f} conex√µes/conceito")
        print(f"  M√≠nima:  {min(conn_counts)}")
        print(f"  M√°xima:  {max(conn_counts)}")
        
        # Top conceitos mais conectados
        most_connected = sorted(
            [(c['id'], c['name'], len(c.get('connections', []))) for c in concepts],
            key=lambda x: -x[2]
        )[:10]
        
        print(f"\n  Top 10 mais conectados:")
        for cid, name, count in most_connected:
            print(f"    ‚Ä¢ {name} ({count})")


def check_relations_integrity(concepts, relations):
    """Verifica integridade das rela√ß√µes"""
    print_section("6. VERIFICA√á√ÉO DE RELA√á√ïES")
    
    all_ids = set(c['id'] for c in concepts)
    
    issues = []
    for rel in relations:
        if rel['from'] not in all_ids:
            issues.append(f"‚ùå ID origem n√£o existe: {rel['from']} ‚Üí {rel['to']}")
        if rel['to'] not in all_ids:
            issues.append(f"‚ùå ID destino n√£o existe: {rel['from']} ‚Üí {rel['to']}")
        if rel['from'] == rel['to']:
            issues.append(f"‚ö†Ô∏è  Auto-rela√ß√£o: {rel['from']}")
    
    if issues:
        print(f"‚ö†Ô∏è  {len(issues)} problemas encontrados:")
        for issue in issues[:20]:
            print(f"   {issue}")
        if len(issues) > 20:
            print(f"   ... e mais {len(issues) - 20} problemas")
    else:
        print(f"‚úÖ Todas as {len(relations)} rela√ß√µes s√£o v√°lidas")
    
    # Verificar duplicatas
    seen = set()
    duplicates = 0
    for rel in relations:
        key = (rel['from'], rel['to'])
        if key in seen:
            duplicates += 1
        seen.add(key)
    
    if duplicates > 0:
        print(f"‚ö†Ô∏è  {duplicates} rela√ß√µes duplicadas")
    else:
        print(f"‚úÖ Nenhuma rela√ß√£o duplicada")
    
    return len(issues) == 0 and duplicates == 0


def analyze_relation_verbs(relations):
    """Analisa distribui√ß√£o de verbos nas rela√ß√µes"""
    print_section("7. AN√ÅLISE DE VERBOS SEM√ÇNTICOS")
    
    verb_counts = Counter(r['name'] for r in relations)
    
    print(f"Total de rela√ß√µes: {len(relations)}")
    print(f"Verbos √∫nicos: {len(verb_counts)}\n")
    
    print("Top 15 verbos mais usados:")
    for verb, count in verb_counts.most_common(15):
        pct = (count / len(relations) * 100) if relations else 0
        print(f"  {verb:25s}: {count:4d} ({pct:5.1f}%)")


def compare_with_literature(concepts, referencias):
    """Compara conceitos do rizoma com literatura"""
    print_section("8. COMPARA√á√ÉO COM LITERATURA")
    
    # Conceitos na literatura
    lit_concepts = set()
    for ref in referencias:
        for c in ref.get('conceitos', []):
            lit_concepts.add(c)
    
    # Conceitos no rizoma
    riz_concepts = set(c['name'] for c in concepts)
    
    # Conceitos produzidos (n√£o mapeados na literatura)
    produced = riz_concepts - lit_concepts
    
    print(f"üìö Conceitos na literatura: {len(lit_concepts)}")
    print(f"üåê Conceitos no rizoma: {len(riz_concepts)}")
    print(f"‚ú® Conceitos produzidos: {len(produced)} ({len(produced)/len(riz_concepts)*100:.1f}%)")
    print(f"üîó Conceitos mapeados: {len(riz_concepts & lit_concepts)}")


def print_final_statistics(concepts, relations):
    """Imprime estat√≠sticas finais"""
    print_section("ESTAT√çSTICAS FINAIS")
    
    print(f"üìö Conceitos: {len(concepts)}")
    print(f"üîó Rela√ß√µes: {len(relations)}")
    print(f"üéØ Verbos √∫nicos: {len(set(r['name'] for r in relations))}")
    print(f"üìä Camadas: {len(CANONICAL_LAYERS)}")
    
    conn_counts = [len(c.get('connections', [])) for c in concepts]
    if conn_counts:
        print(f"üåê M√©dia de conex√µes: {sum(conn_counts)/len(conn_counts):.1f}")


def main():
    """Fun√ß√£o principal"""
    print("\n" + "="*60)
    print("  ATUALIZA√á√ÉO E VALIDA√á√ÉO DA ONTOLOGIA CRIOS")
    print("="*60)
    
    # Carregar dados
    print("\nCarregando arquivos...")
    try:
        concepts = load_json(CONCEPTS_FILE)
        relations = load_json(RELATIONS_FILE)
        referencias = load_json(REFERENCIAS_FILE)
        print(f"‚úÖ {len(concepts)} conceitos carregados")
        print(f"‚úÖ {len(relations)} rela√ß√µes carregadas")
        print(f"‚úÖ {len(referencias)} refer√™ncias carregadas")
    except Exception as e:
        print(f"‚ùå Erro ao carregar arquivos: {e}")
        sys.exit(1)
    
    # Executar verifica√ß√µes
    all_ok = True
    
    all_ok &= check_duplicates(concepts)
    all_ok &= check_required_fields(concepts)
    all_ok &= check_orphan_references(concepts)
    analyze_layer_distribution(concepts)
    analyze_connections(concepts)
    all_ok &= check_relations_integrity(concepts, relations)
    analyze_relation_verbs(relations)
    compare_with_literature(concepts, referencias)
    
    # Estat√≠sticas finais
    print_final_statistics(concepts, relations)
    
    # Resultado final
    print("\n" + "="*60)
    if all_ok:
        print("  ‚úÖ VALIDA√á√ÉO COMPLETA: SISTEMA √çNTEGRO")
    else:
        print("  ‚ö†Ô∏è  VALIDA√á√ÉO COMPLETA: PROBLEMAS ENCONTRADOS")
    print("="*60 + "\n")
    
    return 0 if all_ok else 1


if __name__ == '__main__':
    sys.exit(main())
